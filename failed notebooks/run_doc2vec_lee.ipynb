{"cells":[{"cell_type":"markdown","source":["# Import libs"],"metadata":{"id":"yD9OCmzq5GTX"}},{"cell_type":"code","execution_count":63,"metadata":{"id":"V4TaX5wsz1dW","executionInfo":{"status":"ok","timestamp":1646967057027,"user_tz":-60,"elapsed":208,"user":{"displayName":"Louis Paulet","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gij5eO9YnLcZymYugppl-19R063Z6NgfbR8tXuhxTA=s64","userId":"11821616018910227775"}}},"outputs":[],"source":["%matplotlib inline"]},{"cell_type":"code","execution_count":64,"metadata":{"id":"g-Nr50Mez1da","executionInfo":{"status":"ok","timestamp":1646967057246,"user_tz":-60,"elapsed":2,"user":{"displayName":"Louis Paulet","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gij5eO9YnLcZymYugppl-19R063Z6NgfbR8tXuhxTA=s64","userId":"11821616018910227775"}}},"outputs":[],"source":["import logging\n","logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"]},{"cell_type":"code","execution_count":65,"metadata":{"id":"f-aAD87hz1dd","executionInfo":{"status":"ok","timestamp":1646967057249,"user_tz":-60,"elapsed":5,"user":{"displayName":"Louis Paulet","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gij5eO9YnLcZymYugppl-19R063Z6NgfbR8tXuhxTA=s64","userId":"11821616018910227775"}}},"outputs":[],"source":["import os\n","import gensim\n","# Set file names for train and test data\n","test_data_dir = os.path.join(gensim.__path__[0], 'test', 'test_data')\n","lee_train_file = os.path.join(test_data_dir, 'lee_background.cor')\n","lee_test_file = os.path.join(test_data_dir, 'lee.cor')"]},{"cell_type":"markdown","source":["# Create the dataset"],"metadata":{"id":"cSZfGfP61ftI"}},{"cell_type":"code","source":["import pandas as pd"],"metadata":{"id":"4XKHhEY_2HQM","executionInfo":{"status":"ok","timestamp":1646967057249,"user_tz":-60,"elapsed":5,"user":{"displayName":"Louis Paulet","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gij5eO9YnLcZymYugppl-19R063Z6NgfbR8tXuhxTA=s64","userId":"11821616018910227775"}}},"execution_count":66,"outputs":[]},{"cell_type":"code","source":["balanced_dataset = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/ressources/keywi/landing_title_dataset.csv')\n","balanced_dataset['title'] = balanced_dataset.title.astype(str)"],"metadata":{"id":"oTaDrg771vr6","executionInfo":{"status":"ok","timestamp":1646967057622,"user_tz":-60,"elapsed":378,"user":{"displayName":"Louis Paulet","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gij5eO9YnLcZymYugppl-19R063Z6NgfbR8tXuhxTA=s64","userId":"11821616018910227775"}}},"execution_count":67,"outputs":[]},{"cell_type":"code","source":["train_df = balanced_dataset.sample(frac=0.5)\n","test_df = balanced_dataset[~balanced_dataset.index.isin(train_df.index)]"],"metadata":{"id":"WLugESOI2RV0","executionInfo":{"status":"ok","timestamp":1646967057622,"user_tz":-60,"elapsed":23,"user":{"displayName":"Louis Paulet","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gij5eO9YnLcZymYugppl-19R063Z6NgfbR8tXuhxTA=s64","userId":"11821616018910227775"}}},"execution_count":68,"outputs":[]},{"cell_type":"code","source":["with open(\"train_file.txt\", 'w') as file:\n","    file.write('\\n'.join(train_df.title))\n","with open(\"test_file.txt\", 'w') as file:\n","    file.write('\\n'.join(test_df.title))"],"metadata":{"id":"hy19ulbx3jIw","executionInfo":{"status":"ok","timestamp":1646967057623,"user_tz":-60,"elapsed":23,"user":{"displayName":"Louis Paulet","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gij5eO9YnLcZymYugppl-19R063Z6NgfbR8tXuhxTA=s64","userId":"11821616018910227775"}}},"execution_count":69,"outputs":[]},{"cell_type":"code","source":["# from google.colab import files\n","# files.download(lee_test_file)"],"metadata":{"id":"1JEk1qwz1ssF","executionInfo":{"status":"ok","timestamp":1646967057623,"user_tz":-60,"elapsed":23,"user":{"displayName":"Louis Paulet","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gij5eO9YnLcZymYugppl-19R063Z6NgfbR8tXuhxTA=s64","userId":"11821616018910227775"}}},"execution_count":70,"outputs":[]},{"cell_type":"code","source":["lee_train_file = '/content/train_file.txt'\n","lee_test_file = '/content/test_file.txt'"],"metadata":{"id":"_4yKvHeP1ego","executionInfo":{"status":"ok","timestamp":1646967057623,"user_tz":-60,"elapsed":23,"user":{"displayName":"Louis Paulet","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gij5eO9YnLcZymYugppl-19R063Z6NgfbR8tXuhxTA=s64","userId":"11821616018910227775"}}},"execution_count":71,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3k2l2ecEz1dd"},"source":["Define a Function to Read and Preprocess Text\n","---------------------------------------------\n","\n","Below, we define a function to:\n","\n","- open the train/test file (with latin encoding)\n","- read the file line-by-line\n","- pre-process each line (tokenize text into individual words, remove punctuation, set to lowercase, etc)\n","\n","The file we're reading is a **corpus**.\n","Each line of the file is a **document**.\n","\n",".. Important::\n","  To train the model, we'll need to associate a tag/number with each document\n","  of the training corpus. In our case, the tag is simply the zero-based line\n","  number.\n","\n","\n"]},{"cell_type":"code","execution_count":72,"metadata":{"id":"ljsxZZzJz1de","executionInfo":{"status":"ok","timestamp":1646967057623,"user_tz":-60,"elapsed":23,"user":{"displayName":"Louis Paulet","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gij5eO9YnLcZymYugppl-19R063Z6NgfbR8tXuhxTA=s64","userId":"11821616018910227775"}}},"outputs":[],"source":["import smart_open\n","\n","def read_corpus(fname, tokens_only=False):\n","    with smart_open.open(fname, encoding=\"iso-8859-1\") as f:\n","        for i, line in enumerate(f):\n","            tokens = gensim.utils.simple_preprocess(line)\n","            if tokens_only:\n","                yield tokens\n","            else:\n","                # For training data, add tags\n","                yield gensim.models.doc2vec.TaggedDocument(tokens, [i])\n","\n","train_corpus = list(read_corpus(lee_train_file))\n","test_corpus = list(read_corpus(lee_test_file, tokens_only=True))"]},{"cell_type":"markdown","metadata":{"id":"jn-UBPSfz1de"},"source":["Let's take a look at the training corpus\n","\n","\n"]},{"cell_type":"code","execution_count":73,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0a7Qtey8z1df","executionInfo":{"status":"ok","timestamp":1646967057624,"user_tz":-60,"elapsed":23,"user":{"displayName":"Louis Paulet","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gij5eO9YnLcZymYugppl-19R063Z6NgfbR8tXuhxTA=s64","userId":"11821616018910227775"}},"outputId":"618dde99-7d14-44dd-d112-26cf0bac0251"},"outputs":[{"output_type":"stream","name":"stdout","text":["[TaggedDocument(words=['gratis', 'hoortestdagen'], tags=[0]), TaggedDocument(words=['westerop', 'ko', 'kapsalon', 'santpoortzuid'], tags=[1])]\n"]}],"source":["print(train_corpus[:2])"]},{"cell_type":"markdown","metadata":{"id":"L7a4gVWuz1df"},"source":["And the testing corpus looks like this:\n","\n","\n"]},{"cell_type":"code","execution_count":74,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lJb981mjz1dg","executionInfo":{"status":"ok","timestamp":1646967057624,"user_tz":-60,"elapsed":16,"user":{"displayName":"Louis Paulet","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gij5eO9YnLcZymYugppl-19R063Z6NgfbR8tXuhxTA=s64","userId":"11821616018910227775"}},"outputId":"cf8c114b-1f2a-4dc8-f276-c108330a3663"},"outputs":[{"output_type":"stream","name":"stdout","text":["[['contact', 'gemeente', 'groningen'], ['sign', 'in', 'linkedin']]\n"]}],"source":["print(test_corpus[:2])"]},{"cell_type":"markdown","metadata":{"id":"2Zg91Wqyz1dg"},"source":["Notice that the testing corpus is just a list of lists and does not contain\n","any tags.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"O043FzOXz1dg"},"source":["Training the Model\n","------------------\n","\n","Now, we'll instantiate a Doc2Vec model with a vector size with 50 dimensions and\n","iterating over the training corpus 40 times. We set the minimum word count to\n","2 in order to discard words with very few occurrences. (Without a variety of\n","representative examples, retaining such infrequent words can often make a\n","model worse!) Typical iteration counts in the published `Paragraph Vector paper <https://cs.stanford.edu/~quocle/paragraph_vector.pdf>`__\n","results, using 10s-of-thousands to millions of docs, are 10-20. More\n","iterations take more time and eventually reach a point of diminishing\n","returns.\n","\n","However, this is a very very small dataset (300 documents) with shortish\n","documents (a few hundred words). Adding training passes can sometimes help\n","with such small datasets.\n","\n","\n"]},{"cell_type":"code","execution_count":75,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iIvamKcmz1dh","executionInfo":{"status":"ok","timestamp":1646967057624,"user_tz":-60,"elapsed":12,"user":{"displayName":"Louis Paulet","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gij5eO9YnLcZymYugppl-19R063Z6NgfbR8tXuhxTA=s64","userId":"11821616018910227775"}},"outputId":"66f932e4-e8b4-40ee-c1fa-d44f6cea9d38"},"outputs":[{"output_type":"stream","name":"stderr","text":["2022-03-11 02:50:57,321 : WARNING : consider setting layer size to a multiple of 4 for greater performance\n"]}],"source":["model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=40)"]},{"cell_type":"markdown","metadata":{"id":"hRS9YEvVz1dh"},"source":["Build a vocabulary\n","\n"]},{"cell_type":"code","execution_count":76,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xid0qmM5z1dh","executionInfo":{"status":"ok","timestamp":1646967057924,"user_tz":-60,"elapsed":309,"user":{"displayName":"Louis Paulet","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gij5eO9YnLcZymYugppl-19R063Z6NgfbR8tXuhxTA=s64","userId":"11821616018910227775"}},"outputId":"1eab72d6-0599-4229-a7e2-ec3120a2e194"},"outputs":[{"output_type":"stream","name":"stderr","text":["2022-03-11 02:50:57,335 : INFO : collecting all words and their counts\n","2022-03-11 02:50:57,338 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n","2022-03-11 02:50:57,347 : INFO : collected 2420 word types and 1027 unique tags from a corpus of 1027 examples and 5206 words\n","2022-03-11 02:50:57,350 : INFO : Loading a fresh vocabulary\n","2022-03-11 02:50:57,363 : INFO : effective_min_count=2 retains 650 unique words (26% of original 2420, drops 1770)\n","2022-03-11 02:50:57,364 : INFO : effective_min_count=2 leaves 3436 word corpus (66% of original 5206, drops 1770)\n","2022-03-11 02:50:57,373 : INFO : deleting the raw counts dictionary of 2420 items\n","2022-03-11 02:50:57,388 : INFO : sample=0.001 downsamples 57 most-common words\n","2022-03-11 02:50:57,390 : INFO : downsampling leaves estimated 2435 word corpus (70.9% of prior 3436)\n","2022-03-11 02:50:57,397 : INFO : estimated required memory for 650 words and 50 dimensions: 790400 bytes\n","2022-03-11 02:50:57,404 : INFO : resetting layer weights\n"]}],"source":["model.build_vocab(train_corpus)"]},{"cell_type":"markdown","metadata":{"id":"GeaWdwoFz1di"},"source":["Essentially, the vocabulary is a dictionary (accessible via\n","``model.wv.vocab``\\ ) of all of the unique words extracted from the training\n","corpus along with the count (e.g., ``model.wv.vocab['penalty'].count`` for\n","counts for the word ``penalty``\\ ).\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ND2dVYndz1di"},"source":["Next, train the model on the corpus.\n","If the BLAS library is being used, this should take no more than 3 seconds.\n","If the BLAS library is not being used, this should take no more than 2\n","minutes, so use BLAS if you value your time.\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TGtPaprCz1di"},"outputs":[],"source":["model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)"]},{"cell_type":"markdown","metadata":{"id":"5pMYvUjMz1di"},"source":["Now, we can use the trained model to infer a vector for any piece of text\n","by passing a list of words to the ``model.infer_vector`` function. This\n","vector can then be compared with other vectors via cosine similarity.\n","\n","\n"]},{"cell_type":"code","execution_count":78,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HwNWNdc2z1di","executionInfo":{"status":"ok","timestamp":1646967060658,"user_tz":-60,"elapsed":11,"user":{"displayName":"Louis Paulet","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gij5eO9YnLcZymYugppl-19R063Z6NgfbR8tXuhxTA=s64","userId":"11821616018910227775"}},"outputId":"f4517cb9-3fca-4544-f787-67273b1435a3"},"outputs":[{"output_type":"stream","name":"stdout","text":["[ 0.11821076  0.03322035 -0.06197895  0.12135652  0.20852715  0.04502743\n","  0.05179525 -0.0193969   0.07544646 -0.02888737  0.18293221  0.13676941\n"," -0.06969482 -0.03838854 -0.02144772 -0.10653731 -0.02389352 -0.07344662\n"," -0.12902184  0.08786379 -0.07799368  0.11776935 -0.18422349  0.04785805\n","  0.0651237   0.00181668  0.06122094 -0.21121283  0.1931972   0.00314464\n"," -0.07126746  0.16450453  0.02620476  0.21824433 -0.08971546 -0.13908242\n","  0.01268764 -0.2485366  -0.09876403  0.10125385  0.11163259  0.0685856\n"," -0.04178153 -0.07361675 -0.0867291   0.06208353  0.01660148 -0.06469194\n","  0.10408166 -0.02004464]\n"]}],"source":["vector = model.infer_vector(['only', 'you', 'can', 'prevent', 'forest', 'fires'])\n","print(vector)"]},{"cell_type":"markdown","metadata":{"id":"iq0awLpkz1dj"},"source":["Note that ``infer_vector()`` does *not* take a string, but rather a list of\n","string tokens, which should have already been tokenized the same way as the\n","``words`` property of original training document objects.\n","\n","Also note that because the underlying training/inference algorithms are an\n","iterative approximation problem that makes use of internal randomization,\n","repeated inferences of the same text will return slightly different vectors.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"X2hg3_VZz1dj"},"source":["Assessing the Model\n","-------------------\n","\n","To assess our new model, we'll first infer new vectors for each document of\n","the training corpus, compare the inferred vectors with the training corpus,\n","and then returning the rank of the document based on self-similarity.\n","Basically, we're pretending as if the training corpus is some new unseen data\n","and then seeing how they compare with the trained model. The expectation is\n","that we've likely overfit our model (i.e., all of the ranks will be less than\n","2) and so we should be able to find similar documents very easily.\n","Additionally, we'll keep track of the second ranks for a comparison of less\n","similar documents.\n","\n","\n"]},{"cell_type":"code","execution_count":79,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ERN9orQuz1dj","executionInfo":{"status":"ok","timestamp":1646967067914,"user_tz":-60,"elapsed":7262,"user":{"displayName":"Louis Paulet","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gij5eO9YnLcZymYugppl-19R063Z6NgfbR8tXuhxTA=s64","userId":"11821616018910227775"}},"outputId":"f6ed327c-826e-4134-ab04-03567302f745"},"outputs":[{"output_type":"stream","name":"stderr","text":["2022-03-11 02:51:00,504 : INFO : precomputing L2-norms of doc weight vectors\n"]}],"source":["ranks = []\n","second_ranks = []\n","for doc_id in range(len(train_corpus)):\n","    inferred_vector = model.infer_vector(train_corpus[doc_id].words)\n","    sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))\n","    rank = [docid for docid, sim in sims].index(doc_id)\n","    ranks.append(rank)\n","\n","    second_ranks.append(sims[1])"]},{"cell_type":"markdown","metadata":{"id":"g_EOvd_pz1dj"},"source":["Let's count how each document ranks with respect to the training corpus\n","\n","NB. Results vary between runs due to random seeding and very small corpus\n","\n"]},{"cell_type":"code","execution_count":80,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YfVHzemyz1dk","executionInfo":{"status":"ok","timestamp":1646967067914,"user_tz":-60,"elapsed":15,"user":{"displayName":"Louis Paulet","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gij5eO9YnLcZymYugppl-19R063Z6NgfbR8tXuhxTA=s64","userId":"11821616018910227775"}},"outputId":"542f1ae8-678e-4ec7-b5f1-a06f9d6eb5a9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Counter({0: 44, 1: 18, 2: 15, 3: 11, 4: 9, 5: 9, 9: 6, 95: 5, 6: 5, 822: 5, 8: 5, 224: 4, 21: 4, 7: 4, 56: 4, 701: 4, 83: 4, 10: 4, 17: 4, 48: 4, 14: 4, 367: 4, 471: 4, 145: 4, 351: 4, 740: 4, 466: 3, 735: 3, 604: 3, 134: 3, 41: 3, 336: 3, 27: 3, 1012: 3, 660: 3, 503: 3, 902: 3, 11: 3, 809: 3, 72: 3, 681: 3, 31: 3, 1013: 3, 730: 3, 258: 3, 146: 3, 1014: 3, 461: 3, 60: 3, 620: 3, 685: 3, 42: 3, 57: 3, 453: 3, 375: 3, 712: 3, 13: 3, 344: 3, 286: 3, 19: 3, 12: 3, 661: 3, 229: 3, 409: 3, 1026: 3, 878: 3, 311: 3, 188: 3, 770: 3, 433: 3, 16: 3, 55: 2, 240: 2, 1015: 2, 964: 2, 698: 2, 86: 2, 284: 2, 30: 2, 524: 2, 931: 2, 481: 2, 493: 2, 241: 2, 154: 2, 811: 2, 662: 2, 668: 2, 631: 2, 554: 2, 602: 2, 721: 2, 774: 2, 322: 2, 255: 2, 285: 2, 211: 2, 777: 2, 169: 2, 857: 2, 695: 2, 525: 2, 825: 2, 429: 2, 366: 2, 459: 2, 324: 2, 842: 2, 821: 2, 617: 2, 297: 2, 232: 2, 611: 2, 838: 2, 776: 2, 577: 2, 62: 2, 550: 2, 312: 2, 861: 2, 793: 2, 882: 2, 54: 2, 915: 2, 450: 2, 201: 2, 627: 2, 316: 2, 356: 2, 1001: 2, 371: 2, 183: 2, 1006: 2, 170: 2, 610: 2, 635: 2, 382: 2, 124: 2, 318: 2, 854: 2, 45: 2, 38: 2, 77: 2, 747: 2, 1002: 2, 460: 2, 741: 2, 755: 2, 693: 2, 469: 2, 61: 2, 290: 2, 654: 2, 528: 2, 437: 2, 451: 2, 184: 2, 808: 2, 804: 2, 586: 2, 773: 2, 339: 2, 271: 2, 575: 2, 101: 2, 893: 2, 346: 2, 438: 2, 151: 2, 32: 2, 736: 2, 329: 2, 656: 2, 922: 2, 691: 2, 733: 2, 443: 2, 504: 2, 1009: 2, 323: 2, 794: 2, 74: 2, 797: 2, 441: 2, 334: 2, 26: 2, 262: 2, 18: 2, 799: 2, 686: 2, 899: 2, 374: 2, 978: 2, 758: 2, 578: 2, 221: 2, 300: 2, 679: 2, 795: 2, 568: 2, 667: 2, 178: 2, 283: 2, 65: 2, 724: 2, 119: 2, 759: 2, 996: 2, 305: 2, 126: 2, 315: 2, 222: 2, 369: 2, 78: 2, 130: 2, 105: 2, 420: 2, 1008: 2, 173: 2, 834: 2, 102: 2, 307: 2, 951: 2, 88: 2, 326: 2, 831: 2, 294: 2, 321: 2, 766: 2, 637: 2, 867: 2, 230: 2, 387: 2, 136: 2, 20: 2, 408: 2, 138: 2, 840: 2, 649: 2, 648: 2, 499: 2, 91: 2, 36: 2, 239: 2, 50: 2, 532: 1, 521: 1, 784: 1, 166: 1, 376: 1, 370: 1, 981: 1, 121: 1, 49: 1, 839: 1, 472: 1, 851: 1, 1019: 1, 302: 1, 941: 1, 306: 1, 189: 1, 543: 1, 436: 1, 530: 1, 709: 1, 313: 1, 868: 1, 319: 1, 37: 1, 446: 1, 29: 1, 58: 1, 477: 1, 728: 1, 463: 1, 15: 1, 607: 1, 714: 1, 164: 1, 147: 1, 273: 1, 565: 1, 731: 1, 278: 1, 676: 1, 398: 1, 23: 1, 917: 1, 390: 1, 846: 1, 642: 1, 845: 1, 1021: 1, 892: 1, 228: 1, 816: 1, 559: 1, 345: 1, 488: 1, 352: 1, 509: 1, 836: 1, 457: 1, 768: 1, 761: 1, 900: 1, 548: 1, 633: 1, 287: 1, 987: 1, 483: 1, 191: 1, 946: 1, 482: 1, 1023: 1, 277: 1, 421: 1, 373: 1, 252: 1, 786: 1, 991: 1, 914: 1, 194: 1, 782: 1, 650: 1, 494: 1, 196: 1, 826: 1, 749: 1, 508: 1, 111: 1, 643: 1, 877: 1, 961: 1, 703: 1, 783: 1, 641: 1, 153: 1, 856: 1, 775: 1, 629: 1, 180: 1, 487: 1, 936: 1, 966: 1, 817: 1, 829: 1, 738: 1, 474: 1, 444: 1, 177: 1, 636: 1, 889: 1, 295: 1, 515: 1, 852: 1, 567: 1, 744: 1, 129: 1, 719: 1, 75: 1, 402: 1, 884: 1, 1010: 1, 243: 1, 144: 1, 435: 1, 496: 1, 511: 1, 97: 1, 717: 1, 28: 1, 458: 1, 959: 1, 33: 1, 572: 1, 943: 1, 905: 1, 696: 1, 600: 1, 52: 1, 711: 1, 847: 1, 570: 1, 260: 1, 616: 1, 182: 1, 343: 1, 383: 1, 249: 1, 512: 1, 555: 1, 622: 1, 140: 1, 805: 1, 684: 1, 796: 1, 544: 1, 266: 1, 64: 1, 885: 1, 787: 1, 792: 1, 268: 1, 592: 1, 171: 1, 677: 1, 883: 1, 790: 1, 431: 1, 860: 1, 392: 1, 820: 1, 501: 1, 623: 1, 806: 1, 309: 1, 415: 1, 918: 1, 347: 1, 789: 1, 974: 1, 545: 1, 442: 1, 628: 1, 763: 1, 114: 1, 704: 1, 827: 1, 445: 1, 1016: 1, 353: 1, 430: 1, 213: 1, 746: 1, 104: 1, 256: 1, 486: 1, 141: 1, 942: 1, 197: 1, 206: 1, 652: 1, 630: 1, 168: 1, 456: 1, 181: 1, 259: 1, 837: 1, 514: 1, 293: 1, 406: 1, 850: 1, 391: 1, 455: 1, 220: 1, 1004: 1, 895: 1, 692: 1, 299: 1, 395: 1, 655: 1, 979: 1, 935: 1, 328: 1, 39: 1, 245: 1, 653: 1, 576: 1, 267: 1, 360: 1, 956: 1, 788: 1, 546: 1, 139: 1, 529: 1, 505: 1, 155: 1, 739: 1, 866: 1, 43: 1, 700: 1, 865: 1, 393: 1, 465: 1, 484: 1, 417: 1, 233: 1, 904: 1, 165: 1, 864: 1, 669: 1, 354: 1, 947: 1, 82: 1, 335: 1, 96: 1, 552: 1, 362: 1, 320: 1, 73: 1, 994: 1, 769: 1, 748: 1, 929: 1, 426: 1, 727: 1, 161: 1, 46: 1, 148: 1, 934: 1, 156: 1, 386: 1, 880: 1, 690: 1, 372: 1, 108: 1, 841: 1, 720: 1, 185: 1, 270: 1, 862: 1, 963: 1, 898: 1, 467: 1, 875: 1, 424: 1, 122: 1, 244: 1, 888: 1, 1020: 1, 198: 1, 833: 1, 706: 1, 70: 1, 908: 1, 202: 1, 34: 1, 234: 1, 332: 1, 90: 1, 40: 1, 742: 1, 210: 1, 993: 1, 208: 1, 537: 1, 492: 1, 118: 1, 120: 1, 858: 1, 133: 1, 625: 1, 802: 1, 416: 1, 81: 1, 301: 1, 192: 1, 737: 1, 598: 1, 705: 1, 167: 1, 84: 1, 109: 1, 68: 1, 242: 1, 813: 1, 534: 1, 377: 1, 405: 1, 368: 1, 599: 1, 571: 1, 640: 1, 1024: 1, 673: 1, 379: 1, 664: 1, 519: 1, 348: 1, 51: 1, 910: 1, 106: 1, 99: 1, 478: 1, 887: 1, 702: 1, 413: 1, 163: 1, 135: 1, 762: 1, 965: 1, 879: 1, 59: 1, 66: 1, 995: 1})\n"]}],"source":["import collections\n","\n","counter = collections.Counter(ranks)\n","print(counter)"]},{"cell_type":"markdown","metadata":{"id":"7B_9q9Hwz1dk"},"source":["Basically, greater than 95% of the inferred documents are found to be most\n","similar to itself and about 5% of the time it is mistakenly most similar to\n","another document. Checking the inferred-vector against a\n","training-vector is a sort of 'sanity check' as to whether the model is\n","behaving in a usefully consistent manner, though not a real 'accuracy' value.\n","\n","This is great and not entirely surprising. We can take a look at an example:\n","\n","\n"]},{"cell_type":"code","execution_count":87,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RxcuJvWSz1dk","executionInfo":{"status":"ok","timestamp":1646967553086,"user_tz":-60,"elapsed":4,"user":{"displayName":"Louis Paulet","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gij5eO9YnLcZymYugppl-19R063Z6NgfbR8tXuhxTA=s64","userId":"11821616018910227775"}},"outputId":"0a6e8273-9dcb-43d7-a23f-ad85537c6127"},"outputs":[{"output_type":"stream","name":"stdout","text":["Document (500): «trim en trainingsgroep weesp»\n","\n","SIMILAR/DISSIMILAR DOCS PER MODEL Doc2Vec(dm/m,d50,n5,w5,mc2,s0.001,t3):\n","\n","MOST (831, 0.9356752038002014): «about hba»\n","\n","SECOND-MOST (937, 0.9353445768356323): «about us betway group»\n","\n","MEDIAN (30, 0.9156643748283386): «alles is energie mayawijsheid door fokje ijkema»\n","\n","LEAST (868, -0.8325261473655701): «sign in linkedin»\n","\n"]}],"source":["print('Document ({}): «{}»\\n'.format(doc_id, ' '.join(train_corpus[doc_id].words)))\n","print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n","for label, index in [('MOST', 0), ('SECOND-MOST', 1), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n","    print(u'%s %s: «%s»\\n' % (label, sims[index], ' '.join(train_corpus[sims[index][0]].words)))"]},{"cell_type":"markdown","metadata":{"id":"1NCcp2hqz1dk"},"source":["Notice above that the most similar document (usually the same text) is has a\n","similarity score approaching 1.0. However, the similarity score for the\n","second-ranked documents should be significantly lower (assuming the documents\n","are in fact different) and the reasoning becomes obvious when we examine the\n","text itself.\n","\n","We can run the next cell repeatedly to see a sampling other target-document\n","comparisons.\n","\n","\n"]},{"cell_type":"code","execution_count":82,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zPw0__yXz1dk","executionInfo":{"status":"ok","timestamp":1646967067915,"user_tz":-60,"elapsed":9,"user":{"displayName":"Louis Paulet","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gij5eO9YnLcZymYugppl-19R063Z6NgfbR8tXuhxTA=s64","userId":"11821616018910227775"}},"outputId":"fbc4eca5-03b9-4471-c9b2-b6c214cfe6d0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Train Document (750): «dc online gambling your gambling guide to the nation capital»\n","\n","Similar Document (997, 0.9765671491622925): «wordpress fout»\n","\n"]}],"source":["# Pick a random document from the corpus and infer a vector from the model\n","import random\n","doc_id = random.randint(0, len(train_corpus) - 1)\n","\n","# Compare and print the second-most-similar document\n","print('Train Document ({}): «{}»\\n'.format(doc_id, ' '.join(train_corpus[doc_id].words)))\n","sim_id = second_ranks[doc_id]\n","print('Similar Document {}: «{}»\\n'.format(sim_id, ' '.join(train_corpus[sim_id[0]].words)))"]},{"cell_type":"markdown","metadata":{"id":"wmxVfGWdz1dl"},"source":["Testing the Model\n","-----------------\n","\n","Using the same approach above, we'll infer the vector for a randomly chosen\n","test document, and compare the document to our model by eye.\n","\n","\n"]},{"cell_type":"code","execution_count":83,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0tD733Qwz1dl","executionInfo":{"status":"ok","timestamp":1646967067915,"user_tz":-60,"elapsed":7,"user":{"displayName":"Louis Paulet","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gij5eO9YnLcZymYugppl-19R063Z6NgfbR8tXuhxTA=s64","userId":"11821616018910227775"}},"outputId":"b628d098-7009-40e6-fc05-d1dd290770be"},"outputs":[{"output_type":"stream","name":"stdout","text":["Test Document (500): «home capability»\n","\n","SIMILAR/DISSIMILAR DOCS PER MODEL Doc2Vec(dm/m,d50,n5,w5,mc2,s0.001,t3):\n","\n","MOST (831, 0.9356752038002014): «about hba»\n","\n","MEDIAN (30, 0.9156643748283386): «alles is energie mayawijsheid door fokje ijkema»\n","\n","LEAST (868, -0.8325261473655701): «sign in linkedin»\n","\n"]}],"source":["# Pick a random document from the test corpus and infer a vector from the model\n","doc_id = random.randint(0, len(test_corpus) - 1)\n","inferred_vector = model.infer_vector(test_corpus[doc_id])\n","sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))\n","\n","# Compare and print the most/median/least similar documents from the train corpus\n","print('Test Document ({}): «{}»\\n'.format(doc_id, ' '.join(test_corpus[doc_id])))\n","print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n","for label, index in [('MOST', 0), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n","    print(u'%s %s: «%s»\\n' % (label, sims[index], ' '.join(train_corpus[sims[index][0]].words)))"]},{"cell_type":"markdown","metadata":{"id":"DIhGTXBtz1dl"},"source":["Conclusion\n","----------\n","\n","Let's review what we've seen in this tutorial:\n","\n","0. Review the relevant models: bag-of-words, Word2Vec, Doc2Vec\n","1. Load and preprocess the training and test corpora (see `core_concepts_corpus`)\n","2. Train a Doc2Vec `core_concepts_model` model using the training corpus\n","3. Demonstrate how the trained model can be used to infer a `core_concepts_vector`\n","4. Assess the model\n","5. Test the model on the test corpus\n","\n","That's it! Doc2Vec is a great way to explore relationships between documents.\n","\n","Additional Resources\n","--------------------\n","\n","If you'd like to know more about the subject matter of this tutorial, check out the links below.\n","\n","* `Word2Vec Paper <https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf>`_\n","* `Doc2Vec Paper <https://cs.stanford.edu/~quocle/paragraph_vector.pdf>`_\n","* `Dr. Michael D. Lee's Website <http://faculty.sites.uci.edu/mdlee>`_\n","* `Lee Corpus <http://faculty.sites.uci.edu/mdlee/similarity-data/>`__\n","* `IMDB Doc2Vec Tutorial <doc2vec-IMDB.ipynb>`_\n","\n","\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"},"colab":{"name":"run_doc2vec_lee.ipynb","provenance":[]}},"nbformat":4,"nbformat_minor":0}